{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:09.159956Z",
     "start_time": "2020-11-09T19:50:08.782753Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "logging.basicConfig(format=\"[%(levelname)s] %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:09.172661Z",
     "start_time": "2020-11-09T19:50:09.162092Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df: pd.DataFrame,\n",
    "                        loglevel: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduce memory usage of dataframe\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        loglevel (str, optional): Set log level : \"DEBUG\", \"INFO\". Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Memory optimized dataframe\n",
    "\n",
    "    \n",
    "    Reference:\n",
    "    [1] Reducing memory usage\n",
    "            https://www.kaggle.com/ragnar123/very-fst-model/notebook\n",
    "    \"\"\"\n",
    "    \n",
    "    if loglevel:\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(loglevel)\n",
    "        \n",
    "\n",
    "    prev_memory = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    dtypes_numerical = [\n",
    "        np.uint8, np.uint16, np.uint32, np.uint64,\\\n",
    "        np.int8, np.int16, np.int32, np.int64,\\\n",
    "        np.float16, np.float32, np.float64, np.float128\n",
    "    ]\n",
    "\n",
    "    for col in list(df.columns):\n",
    "        if df[col].dtypes in dtypes_numerical:\n",
    "\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            col_type = df[col].dtypes\n",
    "            col_type_name = df[col].dtypes.name\n",
    "            \n",
    "            for dtype in dtypes_numerical:\n",
    "                if 'int' in col_type_name:\n",
    "                    min_val_supported = np.iinfo(col_type).min\n",
    "                    max_val_supported = np.iinfo(col_type).max\n",
    "\n",
    "                elif 'float' in col_type_name:\n",
    "                    min_val_supported = np.finfo(col_type).min\n",
    "                    max_val_supported = np.finfo(col_type).max\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                if col_min > min_val_supported and col_max < max_val_supported:\n",
    "                    df[col] = df[col].astype(dtype)\n",
    "                    \n",
    "                    if loglevel == \"DEBUG\":\n",
    "                        logger.info(f\"{col} : changed from {col_type} -> {dtype.__name__}\")\n",
    "                    break\n",
    "\n",
    "    new_memory = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if loglevel:\n",
    "        logger.info(f\"Previous memory usage {prev_memory:.2f} MB\")\n",
    "        logger.info(f\"Optimized memory usage {new_memory:.2f} MB\")\n",
    "    print(f\"Percentage reduction in memory {100*(prev_memory-new_memory)/prev_memory:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:09.275000Z",
     "start_time": "2020-11-09T19:50:09.174907Z"
    },
    "code_folding": [
     24
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(data_root: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load data with reduced memory usage\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Root directory for data files\n",
    "\n",
    "    Returns:\n",
    "        data (dict): dictionary with loaded dataframes\n",
    "        \n",
    "    Reference:\n",
    "        [1] Dataframe dtypes\n",
    "            https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # dtypes for reducing memory usage\n",
    "\n",
    "    CALENDAR_DTYPES = {\n",
    "        \"event_name_1\": \"category\",\n",
    "        \"event_name_2\": \"category\",\n",
    "        \"event_type_1\": \"category\",\n",
    "        \"event_type_2\": \"category\",\n",
    "        \"weekday\": \"category\",\n",
    "    }\n",
    "\n",
    "    SALESTRAIN_DTYPES = {\n",
    "        \"id\": \"category\",\n",
    "        \"item_id\": \"category\",\n",
    "        \"dept_id\": \"category\",\n",
    "        \"cat_id\": \"category\",\n",
    "        \"store_id\": \"category\",\n",
    "        \"state_id\": \"category\"\n",
    "    }\n",
    "\n",
    "    PRICE_DTYPES = {\n",
    "        \"store_id\": \"category\",\n",
    "        \"item_id\": \"category\",\n",
    "    }\n",
    "\n",
    "    # load csv and reduce memory usage\n",
    "\n",
    "    df_cal = pd.read_csv(data_root + \"calendar.csv\", parse_dates = ['date'])\n",
    "    df_cal = reduce_memory_usage(df_cal, None)\n",
    "    df_cal = df_cal.astype(CALENDAR_DTYPES)\n",
    "    df_cal = df_cal.rename(columns={'d':'day'})\n",
    "\n",
    "    df_slt = pd.read_csv(data_root + \"sales_train_validation.csv\")\n",
    "    df_slt = reduce_memory_usage(df_slt, None)\n",
    "    df_slt = df_slt.astype(SALESTRAIN_DTYPES)\n",
    "\n",
    "    df_slp = pd.read_csv(data_root + \"sell_prices.csv\")\n",
    "    df_slp = reduce_memory_usage(df_slp, None)\n",
    "    df_slp = df_slp.astype(PRICE_DTYPES)\n",
    "\n",
    "    df_sub = pd.read_csv(data_root + \"sample_submission.csv\")\n",
    "    df_sub = reduce_memory_usage(df_sub, None)\n",
    "\n",
    "    data = {\n",
    "        \"calendar\": df_cal,\n",
    "        \"sales_train\": df_slt,\n",
    "        \"sell_prices\": df_slp,\n",
    "        \"submission\": df_sub\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:09.530412Z",
     "start_time": "2020-11-09T19:50:09.278146Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/processed/data_memopt.bin\"):\n",
    "    data_memopt = load_data(\"../data/extracted/\")\n",
    "    joblib.dump(data_memopt, filename=\"../data/processed/data_memopt.bin\")\n",
    "else:\n",
    "    data_memopt = joblib.load(filename=\"../data/processed/data_memopt.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:09.540208Z",
     "start_time": "2020-11-09T19:50:09.532427Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpivot_data(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Unpivot the data for sales_train_validation.csv, submissions.csv\n",
    "\n",
    "    Args:\n",
    "        data (str): dictionary with loaded dataframes\n",
    "\n",
    "    Returns:\n",
    "        data (dict): dictionary with unpivoted dataframes\n",
    "    \n",
    "    Reference\n",
    "        [1] Melt operation\n",
    "            https://www.kaggle.com/beezus666/end-to-end-data-wrangling-simple-random-forest\n",
    "    \"\"\"\n",
    "    df_cal = data['calendar']\n",
    "    df_slt = data['sales_train']\n",
    "    df_slp = data['sell_prices']\n",
    "    df_sub = data['submission']\n",
    "\n",
    "    id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "    # unpivot sales_train_validation.csv\n",
    "    df_slt_melted = pd.melt(\n",
    "        df_slt,\n",
    "        id_vars=id_vars,\n",
    "        var_name='day',\n",
    "        value_name='sales',\n",
    "    )\n",
    "\n",
    "    # drop redundant columns from sales_train_validation.csv\n",
    "    df_slt_melted.drop(\n",
    "        ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "    \n",
    "    # unpivot submissions.csv\n",
    "    df_sub_melted = pd.melt(\n",
    "        df_sub,\n",
    "        id_vars = ['id'],\n",
    "        value_vars = df_sub.drop(['id'], axis=1).columns,\n",
    "        var_name = 'day',\n",
    "        value_name='sales'\n",
    "    )\n",
    "    \n",
    "    df_sub_melted['day'] = df_sub_melted['day'].str.replace('F', '')\n",
    "    \n",
    "    # after 1913rd day we need to forecast\n",
    "    df_sub_melted['day'] = pd.to_numeric(df_sub_melted['day'], errors='coerce')\n",
    "    df_sub_melted['day'] += 1913 \n",
    "    df_sub_melted = df_sub_melted.applymap(str)\n",
    "    df_sub_melted['day'] = 'd_'+ df_sub_melted['day'].astype(str)\n",
    "\n",
    "    data_melted = {\n",
    "        \"calendar\": df_cal,\n",
    "        \"sales_train_melted\": df_slt_melted,\n",
    "        \"sell_prices\": df_slp,\n",
    "        \"submission_melted\": df_sub_melted\n",
    "    }\n",
    "\n",
    "    return data_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:14.767285Z",
     "start_time": "2020-11-09T19:50:09.543488Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/processed/data_melted.bin\"):\n",
    "    data_melted = unpivot_data(data_memopt)\n",
    "    joblib.dump(data_melted, filename=\"../data/processed/data_melted.bin\")\n",
    "else:\n",
    "    data_melted = joblib.load(filename=\"../data/processed/data_melted.bin\")\n",
    "    del data_memopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:14.787741Z",
     "start_time": "2020-11-09T19:50:14.769771Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(data: dict, start_day: int = 1000, train_split=.20) -> dict:\n",
    "    \"\"\"\n",
    "    Merge data and prepare train and test sets from unpivoted data\n",
    "\n",
    "    Args:\n",
    "        data (str): dictionary with loaded dataframes\n",
    "\n",
    "    Returns:\n",
    "        data (dict): dictionary with unpivoted dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    df_cal = data['calendar']\n",
    "    df_slt = data['sales_train_melted']\n",
    "    df_slp = data['sell_prices']\n",
    "    df_sub = data['submission_melted']\n",
    "\n",
    "    # getting train test split indices\n",
    "    start_day = 'd_' + str(start_day)\n",
    "    ix_start = train_ix_start = df_slt[df_slt['day'] == start_day].index[0]\n",
    "    ix_end = df_slt.index[-1]\n",
    "    total_samples = ix_end - ix_start + 1\n",
    "    train_ix_end = train_ix_start + int(total_samples * train_split)\n",
    "\n",
    "    # train test split\n",
    "    df_slt['id'] = df_slt['id'].str.replace('_validation', '')\n",
    "    df_train = df_slt.iloc[train_ix_start:train_ix_end]\n",
    "    df_test = df_slt.iloc[train_ix_end:]\n",
    "\n",
    "    # merge with calendar days\n",
    "    df_train = df_train.merge(right=df_cal,\n",
    "                              left_on='day',\n",
    "                              right_on='day',\n",
    "                              how='left')\n",
    "    df_test = df_test.merge(right=df_cal,\n",
    "                            left_on='day',\n",
    "                            right_on='day',\n",
    "                            how='left')\n",
    "\n",
    "    # merging with respective prices\n",
    "    df_slp['id'] = df_slp['item_id'].astype(str) + '_' + df_slp['store_id'].astype(str)\n",
    "    df_slp.drop(['item_id', 'store_id'], axis=1, inplace=True)\n",
    "\n",
    "    # preparing submissions df\n",
    "    df_sub['id'] = df_sub['id'].str.replace('_evaluation', '')\n",
    "    df_sub['id'] = df_sub['id'].str.replace('_validation', '')\n",
    "\n",
    "    df_train = pd.merge(left=df_train,\n",
    "                        right=df_slp,\n",
    "                        left_on=['id', 'wm_yr_wk'],\n",
    "                        right_on=['id', 'wm_yr_wk'])\n",
    "    df_test = pd.merge(left=df_test,\n",
    "                       right=df_slp,\n",
    "                       left_on=['id', 'wm_yr_wk'],\n",
    "                       right_on=['id', 'wm_yr_wk'])\n",
    "    \n",
    "    df_sub = df_sub.merge(df_cal, left_on='day', right_on='day', how='left')\n",
    "\n",
    "    data_prep = {\"train\": df_train, \"test\": df_test, \"submission\": df_sub}\n",
    "\n",
    "    return data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:17.190244Z",
     "start_time": "2020-11-09T19:50:14.790566Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/processed/data_prep.bin\"):\n",
    "    data_prep = prepare_data(data_melted, start_day=1800, train_split=0.80)\n",
    "    joblib.dump(data_prep, filename=\"../data/processed/data_prep.bin\")\n",
    "else:\n",
    "    data_prep = joblib.load(filename=\"../data/processed/data_prep.bin\")\n",
    "    del data_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:17.217822Z",
     "start_time": "2020-11-09T19:50:17.193152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4417232, 17), (717036, 17), (1707440, 16))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep['train'].shape, data_prep['test'].shape, data_prep['submission'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:17.380852Z",
     "start_time": "2020-11-09T19:50:17.221649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'day', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month',\n",
       "       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep['train'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:17.515018Z",
     "start_time": "2020-11-09T19:50:17.385707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'day', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month',\n",
       "       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep['test'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:17.658994Z",
     "start_time": "2020-11-09T19:50:17.517867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'day', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month',\n",
       "       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep['submission'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T19:50:19.070148Z",
     "start_time": "2020-11-09T19:50:17.663049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4417232, 17),\n",
       " id              4417232\n",
       " day             4417232\n",
       " sales           4417232\n",
       " date            4417232\n",
       " wm_yr_wk        4417232\n",
       " weekday         4417232\n",
       " wday            4417232\n",
       " month           4417232\n",
       " year            4417232\n",
       " event_name_1     512530\n",
       " event_type_1     512530\n",
       " event_name_2          0\n",
       " event_type_2          0\n",
       " snap_CA         4417232\n",
       " snap_TX         4417232\n",
       " snap_WI         4417232\n",
       " sell_price      4417232\n",
       " dtype: int64,\n",
       " (717036, 17),\n",
       " id              717036\n",
       " day             717036\n",
       " sales           717036\n",
       " date            717036\n",
       " wm_yr_wk        717036\n",
       " weekday         717036\n",
       " wday            717036\n",
       " month           717036\n",
       " year            717036\n",
       " event_name_1         0\n",
       " event_type_1         0\n",
       " event_name_2         0\n",
       " event_type_2         0\n",
       " snap_CA         717036\n",
       " snap_TX         717036\n",
       " snap_WI         717036\n",
       " sell_price      717036\n",
       " dtype: int64,\n",
       " (1707440, 16),\n",
       " id              1707440\n",
       " day             1707440\n",
       " sales           1707440\n",
       " date            1707440\n",
       " wm_yr_wk        1707440\n",
       " weekday         1707440\n",
       " wday            1707440\n",
       " month           1707440\n",
       " year            1707440\n",
       " event_name_1     243920\n",
       " event_type_1     243920\n",
       " event_name_2          0\n",
       " event_type_2          0\n",
       " snap_CA         1707440\n",
       " snap_TX         1707440\n",
       " snap_WI         1707440\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = data_prep['train']\n",
    "df_test = data_prep['test']\n",
    "df_submission = data_prep['submission']\n",
    "\n",
    "# check if any missing values are present\n",
    "df_train.shape, df_train.count(), df_test.shape, df_test.count(), df_submission.shape, df_submission.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
