{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"color:red; font-size: 50px \" > Research </p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 6 Powerful Feature Engineering Techniques For Time Series Data (using Python)\n",
    "\n",
    "[Source](https://www.analyticsvidhya.com/blog/2019/12/6-powerful-feature-engineering-techniques-time-series/)\n",
    "\n",
    "## Feature engineering:\n",
    "1. Date Features\n",
    "    - Extract day, month, year, day_of_week_num, day_of_week_name\n",
    "\n",
    "1. Lag Features\n",
    "    - Previous day's prices are important to make decisions about the future\n",
    "    - Shift series ahead by 1, 2, ... k and use them as features\n",
    "1. Rolling window features\n",
    "    - Do Feature engineering on rolling window\n",
    "    - e.g Rolling mean with period of 7 days\n",
    "1. Domain specific features\n",
    "    - Adding domain specific features can improve the quality of predictions\n",
    "    - e.g adding features specific to stock market such as technical indicators will help us better predict value of stock\n",
    "    \n",
    "## Comments:\n",
    "1. These features can be added to our feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Some simple forecasting methods\n",
    "\n",
    "[Source](https://otexts.com/fpp2/simple-methods.html)\n",
    "\n",
    "1. Average method\n",
    "    - We forecast future values as equal to average of all time series\n",
    "    - Let historical data be $y_{1},\\dots,y_{T}$\n",
    "    >- $\\hat{y}_{T+h|T} = \\bar{y} = (y_{1}+\\dots+y_{T})/T$\n",
    "\n",
    "1. Na誰ve method\n",
    "    - For na誰ve forecasts, we simply set all forecasts to be the value of the last observation.\n",
    "    - This method works very well for financial time series\n",
    "    >- $\\hat{y}_{T+h|T} = y_{T}$\n",
    "\n",
    "1. Seasonal na誰ve method\n",
    "    - In this method we set forecast to be equal to last observed value in the season\n",
    "    >- $\\hat{y}_{T+h|T} = y_{T+h-m(k+1)}$\n",
    "    \n",
    "## Comments:\n",
    "1. These are na誰ve methods for forecasting\n",
    "1. We can use them as base line models for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Complete Guide to Time Series Analysis and Forecasting\n",
    "\n",
    "[Source]( https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)\n",
    "\n",
    "## Overview:\n",
    "This source explains various terms associated with time series such as:\n",
    "1. Stationarity\n",
    "    - A time series is said to be stationary if its statistical properties do not change over time.\n",
    "2. Seasonality\n",
    "    - Seasonality refers to periodic fluctuations.\n",
    "3. Autocorrelation\n",
    "    - Autocorrelation is a mathematical representation of the degree of similarity between a given time series and a lagged version of itself over successive time intervals.\n",
    "\n",
    "It also explains briefly about various ways to model time series:\n",
    "1. Moving average\n",
    "2. Exponential moving average\n",
    "3. Double exponential moving average\n",
    "4. Triple exponential moving average\n",
    "5. ARIMA method (This is discussed in later part in detail)\n",
    "\n",
    "## Comments:\n",
    "1. Time series has various properties, understanding these properties are very important in order to apply techniques specific to that time series\n",
    "2. Time series can be modelled using Moving averages. \n",
    "    - But we can try using these as features for machine learning model.\n",
    "    - Using various moving averages might add value to feature set\n",
    "    - Similarly we can use moving averages on lag features to add to feature set\n",
    "3. We can experiment with this feature set for modelling purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LSTM-MSNET\n",
    "\n",
    "[Source](https://arxiv.org/pdf/1909.04293.pdf)\n",
    " \n",
    "## Overview:\n",
    "\n",
    "This paper discusses about a new architecture LSTM-MSNET which deals with multiseasonality in time series. The paper deals with Time Series Forecasting and proposes a method which takes inspiration from both the Statistical and Deep Learning World.\n",
    "\n",
    "The paper is divided into 4 parts:\n",
    "1. Time series preprocessing\n",
    "    - Normalization \n",
    "    - Variance stabilization using log\n",
    "    - Moving window transformation\n",
    "2. Seasonal Decomposition\n",
    "    - Multiple STL Decomposition (MSTL)\n",
    "    - Seasonal-Trend decomposition by Regression (STR)\n",
    "    - Trigonometric, Box-Cox, ARMA, Trend, Seasonal (TBATS)\n",
    "    - Prophet\n",
    "    - Fourier Transformation\n",
    "3. Modelling using LSTM MSNet\n",
    "4. Deseasonalized approach\n",
    "\n",
    "We are will be focusing more on 1. and 2. \n",
    "\n",
    "## Main ideas:\n",
    "1. Time series preprocessing for stability can be done using methods discussed in this paper\n",
    "1. Deseasonalising time series prior to modelling is useful\n",
    "\n",
    "## Comments:\n",
    "1. We will use the preprocessing step for our time series data\n",
    "1. We will try using deseasonalising before modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"color:blue; font-size: 40px \" > My unique ideas </p>\n",
    "<br>\n",
    "\n",
    "1. Using technical indicators for feature engineering\n",
    "1. Using amount of sale of certain item as volume\n",
    "1. Creating multi seasonal time series\n",
    "    - Say we have a time series y1, y2, y3, ... yk\n",
    "    - Use y1, y7, y14, ... to create weekly seasonal time series\n",
    "    - Similarly create different seasonalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style=\"color:green; font-size: 40px \" > Existing Solutions </p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Very fst model [kernel]\n",
    "\n",
    "[Source](https://www.kaggle.com/ragnar123/very-fst-model/notebook)\n",
    "\n",
    "\n",
    "## Data preparation:\n",
    "1. Reducing Memory usage of dataframe\n",
    ">- Change datatypes\n",
    ">- Melt function usage\n",
    "1. Join and prepare data for training\n",
    "\n",
    "## Feature engineering:\n",
    "1. Lag features\n",
    ">- Shift columns ahead by a certain period to create Lag in features\n",
    "1. Rolling demand features\n",
    ">- Rolling features calculated on demand of items\n",
    ">- Rolling mean, std, skewness, kurtosis\n",
    "1. Rolling price features\n",
    ">- Rolling features calculated on price of items\n",
    ">- Rolling max price, max price change in lag prices\n",
    "1. Use date features\n",
    ">- Date day\n",
    ">- Date month\n",
    ">- Date year\n",
    ">- Week day\n",
    "\n",
    "\n",
    "## Modelling:\n",
    "1. LGBM used for training simple baseline model\n",
    "\n",
    "## Key takeaways:\n",
    "1. Memory reduction techniques\n",
    "1. Lag features\n",
    "1. Rolling features\n",
    "1. Using date features and LGBM we can extract the importance of time as a feature\n",
    "1. Attained RMSE of 2.119324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. M5 First Public Notebook Under 0.50 [kernel]\n",
    "\n",
    "[Source]( https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50)\n",
    "\n",
    "\n",
    "## Data preparation:\n",
    "1. Create training data using merge\n",
    "1. Reduce memory using melt operation\n",
    "1. Changed data types to reduce memory\n",
    "\n",
    "\n",
    "## Feature engineering:\n",
    "1. Lag features\n",
    ">- Shift columns ahead by a certain period to create Lag in features\n",
    "1. Rolling features on Lag features\n",
    ">- Using rolling features such as rolling mean on Lag features for different windows\n",
    "1. Use date features\n",
    ">- Date day\n",
    ">- Date month\n",
    ">- Date year\n",
    ">- Week day\n",
    ">- Week month\n",
    ">- Quarter\n",
    "1. Use of FIRST_DAY parameter\n",
    ">- This was done to avoid memory overflow\n",
    "1. Categorical features\n",
    ">- item_id, dept_id,store_id, cat_id, state_id, event_name_1, event_name_2, event_type_1, event_type_2 used as it is\n",
    "\n",
    "\n",
    "## Modelling:\n",
    "1. LGBM used for training model to predict sales\n",
    "1. Attained score below 0.50\n",
    "1. Used RMSE as metric to monitor performance of model\n",
    "1. RMSE attained 2.32389\n",
    "1. Parameters to train lgb\n",
    "```python\n",
    "params = {\n",
    "    \"objective\" : \"poisson\",\n",
    "    \"metric\" :\"rmse\",\n",
    "    \"force_row_wise\" : True,\n",
    "    \"learning_rate\" : 0.075,\n",
    "    \"sub_row\" : 0.75,\n",
    "    \"bagging_freq\" : 1,\n",
    "    \"lambda_l2\" : 0.1,\n",
    "    \"metric\": [\"rmse\"],\n",
    "    'num_iterations' : 1200,\n",
    "    'num_leaves': 128,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "}\n",
    "```\n",
    "\n",
    "## Key takeaways:\n",
    "- Usage of new date features : Quarter and week month \n",
    "- lag_*period*: feature lag_*period* this feature gives better context of a *period* by shifting a feature by *period* e.g: lag_7 gives us better context of a week\n",
    "-  The reason for using lagged values of the target variable is to reduce the effect of self-propagating errors through multiple predictions of the same model\n",
    "- Similarly using rolling on lag features gives us very rich information\n",
    "- Suppose we are at day 7 average of sales on day 7 is average of sales from day 1-7 using rolling mean on lag features allows us to capture this information\n",
    "- Looking at parameters we can see that following parameters play important role in training:\n",
    "    - sub_row\n",
    "    - num_iterations\n",
    "    - num_leaves\n",
    "    - min_data_in_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. M5 - Simple FE [kernel]\n",
    "\n",
    "[Source]( https://www.kaggle.com/kyakovlev/m5-simple-fe)\n",
    "\n",
    "\n",
    "## Data preparation:\n",
    "1. Data preparation was done in very similar way like earlier kernels\n",
    "\n",
    "## Feature engineering:\n",
    "1. Basic aggregation for prices dataframe was done on level like [store_id, item_id]\n",
    "1. On calculating min, max values on these aggregations we can use this to normalise price values\n",
    "1. Some momentum based features were added in this kernel\n",
    "1. price_momentum, price_momentum_m (for monthly basis), price_momentum_y (for yearly basis)\n",
    "1. price_nunique, item_nunique: Some items are inflation dependent whereas some are stable, these features capture essence of inflation from the data\n",
    "\n",
    "## Key takeaways:\n",
    "1. Aggregation features was a new approach here\n",
    "1. Aggregated normalization adds new information about values\n",
    "1. Technique for normalising was min_max_scaling\n",
    "1. This kernel introduces us to momentum based features \n",
    "1. price_nunique and item_nunique can turn out to be important features which capture inflation of the items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "Important Kaggle discussion:\n",
    "- [Top discussion](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163414)\n",
    "\n",
    "Forecasting Principles and Practice:\n",
    "- [Notes](https://robjhyndman.com/uwafiles/fpp-notes.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Cut Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading:\n",
    "    - Based on the existing kernels we can see that the problem requires huge amount of memory.\n",
    "    - In order to load dataset we need to reduce the memory usage by changing data types of columns.\n",
    "    - Complete dataset for training can be obtained by joining 3 tables/ csv files : sales_train_validation.csv, calendar.csv, sell_prices.csv .\n",
    "    - If the dataset is very large we will take some part/ subset of the dataset before moving to the cloud platform.\n",
    "    - Also dask can be used if we are working on local machine.\n",
    "\n",
    "1. Exploratory Data Analysis:\n",
    "    - We will do basic time series analysis to get insights about the data.\n",
    "    - We will try to get understanding of distribution of sales.\n",
    "    - How distribution of sales is changing with respect to time (due to occurence of any special events).\n",
    "    - Check how the sales are being affected on weekdays and weekends.\n",
    "    - Check how the sales are being affected on monthly basis, etc.\n",
    "\n",
    "1. Feature Engineering:\n",
    "    - For time series data we can engineer following features in the beginning:\n",
    "        - `Lag features` : These features will help us understand how previous data is affecting current sell prices, we can use multiple lag features with various shifts, eg: lag_7 (7 days shift) will help us understand how prices of goods 7 days before are affecting current prices. Similarly we can use lag_14, lag_28 like features to get understanding of different periods.\n",
    "        - `Rolling features` : Rolling features will try to summarize how changes in a certain window period affect the sales, some features we can extract from rolling window are, rolling mean, rolling standard devivation, with different periods, also we can apply the same on Lag Features.\n",
    "        - `Date features` : Extracting day, month, year, week day, week number, etc will give us understanding of, if it is any special day that will boost sales of a certain item.\n",
    "    \n",
    "    - Some complex features that we can generate in later parts are:\n",
    "        - `Seasonal decomposition`: Decomposing time series into season and trend will help us understand if the time series is sensitive to any certain season or is following any trend. We will break time series into various different periods.\n",
    "        - `Technical indicators`: Some technical indicators from stock market might serve as good features for time series, they are meant to capture essence of complex stock price movemnets.\n",
    "\n",
    "1. Data Normalization and preprocessing:\n",
    "    - Taking log of prices stabilizes the time series.\n",
    "    - We can normalize using window wise normalization dividing each price with sum of all prices in that window\n",
    "    - To handle non stationarity we will take log of these normalized prices\n",
    "    - Normalizing might not be useful if we use non parametric models like decision trees/ random forest, we can skip this step in that case.\n",
    "    \n",
    "1. Creating a data pipeline:\n",
    "    - For training we need to pass data in one of the two formats:\n",
    "        a. Sliding window method - Preferred when data is more and memory is less\n",
    "        b. Growing window method - Preferred when data is less (more robust)\n",
    "    - We will create both the pipelines in order to see which is working well on our dataset\n",
    "\n",
    "1. Creating a baseline model:\n",
    "    - Just like we use Random model to gauge the performance of classification model, here we will use Naive Prediction model\n",
    "    - There are multiple approaches to do so, such as average method, naive method, seasonal naive method.\n",
    "    - We will use the method which gives us least Key performance metric for the case study i.e Weighted Root Mean Squared Scaled Error (RMSSE).\n",
    "    - Say if seasonal naive method gives leas RMSSE then we will use it as a baseline model\n",
    "\n",
    "1. Modelling:\n",
    "    - We will try to model using LightGBM for this case study.\n",
    "    - It is known that Xgboost produces very good results, but training time for Xgboost can be very high.\n",
    "    - We will pass data using data pipelines created, save the weights of model for each iteration.\n",
    "    - And load weights for next iteration (next window of data).\n",
    "    - We will initially do this for a smaller subset of the data see which features are driving the predictions of sales.\n",
    "    - Using RMSE we will monitor the performance of our model.\n",
    "    - On knowing feature importances we will scale down the feature set accordingly.\n",
    "\n",
    "1. Hyper parameter optimization:\n",
    "    - In order to achieve best result we can tune our model using hyperopt library.\n",
    "    - This library uses Bayesian Optimization for getting the best results\n",
    "\n",
    "1. Future improvements:\n",
    "    - In order to improve this solution we can use Deep learning models to model this complex data.\n",
    "    - LSTMS can be used in further stages of this project.\n",
    "    - Training LSTMS can be very complex and might require huge resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
